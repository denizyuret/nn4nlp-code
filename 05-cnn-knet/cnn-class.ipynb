{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Convolutional Sentiment Classification Network"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Knet\n",
    "using Random, Statistics, Printf"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are using the data from Stanford Sentiment Treebank dataset\n",
    "without tree information. First, we initialize our word->id and\n",
    "tag->id collections and insert padding word \"&lt;pad&gt;\" and\n",
    "unknown word \"&lt;unk&gt;\" symbols into word->id collection."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "wdict, tdict = Dict(), Dict()\n",
    "w2i(x) = get!(wdict, x, 1+length(wdict))\n",
    "t2i(x) = get!(tdict, x, 1+length(tdict))\n",
    "PAD = w2i(\"<pad>\")\n",
    "UNK = w2i(\"<unk>\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the data files, each line consists of sentiment and sentence\n",
    "information separated by `|||`."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function readdata(file)\n",
    "    instances = []\n",
    "    for line in eachline(file)\n",
    "        y, x = split(line, \" ||| \")\n",
    "        y, x = t2i(y), w2i.(split(x))\n",
    "        push!(instances, (x,[y]))\n",
    "    end\n",
    "    return instances\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "After reading training data, we redefine ```w2i``` procedure to\n",
    "avoid inserting new words into our vocabulary collection and then\n",
    "read validation data."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "trn = readdata(\"../data/classes/train.txt\")\n",
    "w2i(x) = get(wdict, x, UNK)\n",
    "t2i(x) = tdict[x]\n",
    "nwords, ntags = length(wdict), length(tdict)\n",
    "dev = readdata(\"../data/classes/test.txt\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We begin developing convolutional sentiment classification model.\n",
    "Our model is a stack of five consecutive operations: word embeddings,\n",
    "1-dimensional convolution, max-pooling, ReLU activation and linear\n",
    "prediction layer. First, we define our network,"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "mutable struct CNN\n",
    "    embedding\n",
    "    conv1d\n",
    "    linear\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, we implement the forward propagation and loss calculation,"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function (model::CNN)(x)\n",
    "    windowsize = size(model.conv1d.w, 2)\n",
    "    if windowsize > length(x)\n",
    "        x = vcat(x, [PAD for i = 1:windowsize-length(x)])\n",
    "    end\n",
    "    emb = model.embedding(x)\n",
    "    T, E = size(emb); B = 1\n",
    "    emb = reshape(emb, 1, T, E, B)              # 1, Time, Embedding, Batch\n",
    "    hidden = model.conv1d(emb)                  # 1, Time-1, Nfilters, Batch\n",
    "    hidden = relu.(maximum(hidden, dims=2))     # 1, 1, Nfilters, Batch\n",
    "    hidden = reshape(hidden, size(hidden,3), B) # Nfilters, Batch\n",
    "    output = model.linear(hidden)               # Nclasses, Batch\n",
    "end\n",
    "\n",
    "(model::CNN)(x,y) = nll(model(x),y)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "In order to make our network working, we need to implement ```Embedding```,\n",
    "```Linear``` and ```Conv``` layers,"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "mutable struct Embedding; w; end\n",
    "(layer::Embedding)(x) = layer.w[x, :]\n",
    "Embedding(vocabsize::Int, embedsize::Int) = Embedding(\n",
    "    param(vocabsize, embedsize))\n",
    "\n",
    "\n",
    "mutable struct Linear; w; b; end\n",
    "(layer::Linear)(x) = layer.w * x .+ layer.b\n",
    "Linear(inputsize::Int, outputsize::Int) = Linear(\n",
    "    param(outputsize, inputsize),\n",
    "    param0(outputsize, 1))\n",
    "\n",
    "\n",
    "mutable struct Conv; w; b; end\n",
    "(layer::Conv)(x) = conv4(layer.w, x; stride=1, padding=0) .+ layer.b\n",
    "Conv(embedsize::Int, nfilters::Int, windowsize::Int) = Conv(\n",
    "    param(1, windowsize, embedsize, nfilters),\n",
    "    param0(1, 1, nfilters, 1))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We initialize our model,"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "EMBEDSIZE = 64\n",
    "WINSIZE = 3\n",
    "NFILTERS = 64\n",
    "model = CNN(\n",
    "    Embedding(nwords, EMBEDSIZE),\n",
    "    Conv(EMBEDSIZE, NFILTERS, WINSIZE),\n",
    "    Linear(NFILTERS, ntags))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We implement a validation procedure which computes accuracy and average loss\n",
    "over the entire input data split."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function validate(data)\n",
    "    loss = correct = 0\n",
    "    for (x,y) in data\n",
    "        ŷ = model(x)\n",
    "        loss += nll(ŷ,y)\n",
    "        correct += argmax(Array(ŷ))[1] == y[1]\n",
    "    end\n",
    "    return loss/length(data), correct/length(data)\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, here is the training loop:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function train(nepochs=10)\n",
    "    for epoch=1:nepochs\n",
    "        progress!(adam(model, shuffle(trn)))\n",
    "\n",
    "        trnloss, trnacc = validate(trn)\n",
    "        @printf(\"iter %d: trn loss/sent=%.4f, trn acc=%.4f\\n\",\n",
    "                epoch, trnloss, trnacc)\n",
    "\n",
    "        devloss, devacc = validate(dev)\n",
    "        @printf(\"iter %d: dev loss/sent=%.4f, dev acc=%.4f\\n\",\n",
    "                epoch, devloss, devacc)\n",
    "    end\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Exercises:\n",
    "- The training results in significant overfitting; try using dropout.\n",
    "- The training is too slow; try minibatching."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  },
  "kernelspec": {
   "name": "julia-1.5",
   "display_name": "Julia 1.5.3",
   "language": "julia"
  }
 },
 "nbformat": 4
}
