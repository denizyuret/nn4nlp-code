{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Character Language Modeling using LSTM on Penn treebank"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Importing packages"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Base.Iterators, IterTools, Knet, Printf, LinearAlgebra, StatsBase, Random"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Charset\n",
    "This will hold our set of characters that the model will be able to process."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "struct Charset\n",
    "    c2i::Dict{Any,Int}\n",
    "    i2c::Vector{Any}\n",
    "    eow::Int\n",
    "    mask::Int\n",
    "end\n",
    "\n",
    "function Charset(charset::String; eow=\"\", mask=\"-\")\n",
    "    i2c = [ eow; mask; [ c for c in charset ]  ]\n",
    "    c2i = Dict( c => i for (i, c) in enumerate(i2c))\n",
    "    return Charset(c2i, i2c, c2i[eow], c2i[mask])\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TextReader\n",
    "Here we will read our input files and split into characters line-by-line."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "struct TextReader\n",
    "    file::String\n",
    "    charset::Charset\n",
    "end\n",
    "\n",
    "function Base.iterate(r::TextReader, s=nothing)\n",
    "    s === nothing && (s = open(r.file))\n",
    "    eof(s) && return close(s)\n",
    "    return [ get(r.charset.c2i, c, r.charset.eow) for c in readline(s)], s\n",
    "end\n",
    "\n",
    "Base.IteratorSize(::Type{TextReader}) = Base.SizeUnknown()\n",
    "Base.IteratorEltype(::Type{TextReader}) = Base.HasEltype()\n",
    "Base.eltype(::Type{TextReader}) = Vector{Int}"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### DataIterator\n",
    "In this iterator we will handle iterating over the data, and prepare our models input and output by converting lines of similar lengths into mini training batches."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "struct DataIterator\n",
    "    src::TextReader\n",
    "    batchsize::Int\n",
    "    maxlength::Int\n",
    "    batchmajor::Bool\n",
    "    bucketwidth::Int\n",
    "    buckets::Vector\n",
    "    batchmaker::Function\n",
    "end\n",
    "\n",
    "function DataIterator(src::TextReader; batchmaker = arraybatch, batchsize = 128, maxlength = typemax(Int),\n",
    "                batchmajor = false, bucketwidth = 2, numbuckets = min(128, maxlength ÷ bucketwidth))\n",
    "    # buckets[i] is an array of sentence pairs with similar length\n",
    "    buckets = [ [] for i in 1:numbuckets ]\n",
    "    DataIterator(src, batchsize, maxlength, batchmajor, bucketwidth, buckets, batchmaker)\n",
    "end\n",
    "\n",
    "Base.IteratorSize(::Type{DataIterator}) = Base.SizeUnknown()\n",
    "Base.IteratorEltype(::Type{DataIterator}) = Base.HasEltype()\n",
    "Base.eltype(::Type{DataIterator}) = NTuple{2}\n",
    "\n",
    "function Base.iterate(d::DataIterator, state=nothing)\n",
    "    # When file is finished but buckets are partially full\n",
    "    if state == 0\n",
    "        for i in 1:length(d.buckets)\n",
    "            if length(d.buckets[i]) > 0\n",
    "                batch = d.batchmaker(d, d.buckets[i])\n",
    "                d.buckets[i] = []\n",
    "                return batch, state\n",
    "            end\n",
    "        end\n",
    "        # terminate iteration\n",
    "        return nothing\n",
    "    end\n",
    "\n",
    "    while true\n",
    "        src_next = iterate(d.src, state)\n",
    "\n",
    "        if src_next === nothing\n",
    "            state = 0\n",
    "            return iterate(d, state)\n",
    "        end\n",
    "\n",
    "        (src_word, src_state) = src_next\n",
    "        state = src_state\n",
    "        src_length = length(src_word)\n",
    "\n",
    "        (src_length > d.maxlength) && continue\n",
    "        (src_length < 2) && continue\n",
    "\n",
    "        i = Int(ceil(src_length / d.bucketwidth))\n",
    "        i > length(d.buckets) && (i = length(d.buckets))\n",
    "\n",
    "        push!(d.buckets[i], src_word)\n",
    "        if length(d.buckets[i]) == d.batchsize\n",
    "            batch = d.batchmaker(d, d.buckets[i])\n",
    "            d.buckets[i] = []\n",
    "            return batch, state\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "function arraybatch(d::DataIterator, bucket)\n",
    "    src_eow = d.src.charset.eow\n",
    "    src_lengths = map(x -> length(x), bucket)\n",
    "    max_length = max(src_lengths...)\n",
    "    x = zeros(Int64, length(bucket), max_length + 2)\n",
    "\n",
    "    for (i, v) in enumerate(bucket)\n",
    "        to_be_added = fill(src_eow, max_length - length(v) + 1)\n",
    "        x[i,:] = [src_eow; v; to_be_added]\n",
    "    end\n",
    "\n",
    "    # default d.batchmajor is false\n",
    "    d.batchmajor && (x = x')\n",
    "\n",
    "    # the output in lang. model is same as input sequence but shifted one step\n",
    "    return (x[:, 1:end-1], x[:, 2:end])\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Embedding and Linear layers"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "mmul(w,x) = (w == 1 ? x : w == 0 ? 0 : reshape(w * reshape(x,size(x,1),:), (:, size(x)[2:end]...)))\n",
    "\n",
    "struct Embed; w; end\n",
    "Embed(charsetsize::Int, embedsize::Int) = Embed(param(embedsize, charsetsize))\n",
    "(l::Embed)(x) = l.w[:, x]\n",
    "\n",
    "struct Linear; w; b; end\n",
    "Linear(inputsize::Int, outputsize::Int) = Linear(param(outputsize, inputsize), param0(outputsize))\n",
    "(l::Linear)(x) = mmul(l.w,x) .+ l.b"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### NLL Loss masking function\n",
    "This function masks the training output array by zeros, this way we don't propagate loss from paddings"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function mask(a, pad)\n",
    "    a = copy(a)\n",
    "    for i in 1:size(a, 1)\n",
    "        j = size(a,2)\n",
    "        while a[i, j] == pad && j > 1\n",
    "            if a[i, j - 1] == pad\n",
    "                a[i, j] = 0\n",
    "            end\n",
    "            j -= 1\n",
    "        end\n",
    "    end\n",
    "    return a\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### RNN Language model\n",
    "Our model consists of four layers. Size of their outputs are as the following, where T is sequence length, B is batchsize, H is the hidden size of RNN, and V is vocabulary size, E is the embedding size:\n",
    "\n",
    "* **(T, B)** - Input\n",
    "* **(E, T, B)** - Embedding\n",
    "* **(H, T, B)** - RNN\n",
    "* **(V, T, B)** - Projection"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "struct LModel\n",
    "    srcembed::Embed\n",
    "    rnn::RNN\n",
    "    projection::Linear\n",
    "    dropout::Real\n",
    "    srccharset::Charset\n",
    "end\n",
    "\n",
    "function LModel(hidden::Int, srcembsz::Int, srccharset::Charset; layers=1, dropout=0)\n",
    "\n",
    "    srcembed = Embed(length(srccharset.i2c), srcembsz)\n",
    "    rnn = RNN(srcembsz, hidden; bidirectional=false, numLayers=layers, dropout=dropout)\n",
    "    projection = Linear(hidden, length(srccharset.i2c))\n",
    "\n",
    "    LModel(srcembed, rnn, projection, dropout, srccharset)\n",
    "end\n",
    "\n",
    "function (s::LModel)(src, tgt; average=true)\n",
    "    s.rnn.h, s.rnn.c = 0, 0\n",
    "    srcembed = s.srcembed(src)\n",
    "    rnn_out = s.rnn(srcembed)\n",
    "    dims = size(rnn_out)\n",
    "    output = s.projection(dropout(reshape(rnn_out, dims[1], dims[2] * dims[3]), s.dropout))\n",
    "    scores = reshape(output, size(output, 1), dims[2], dims[3])\n",
    "    nll(scores, mask(tgt, s.srccharset.eow); dims=1, average=average)\n",
    "end\n",
    "\n",
    "function generate(s::LModel; start=\"\", maxlength=30)\n",
    "    s.rnn.h, s.rnn.c = 0, 0\n",
    "    chars = fill(s.srccharset.eow, 1)\n",
    "    start = [ c for c in start ]\n",
    "    starting_index = 1\n",
    "    for i in 1:length(start)\n",
    "        push!(chars, s.srccharset.c2i[start[i]])\n",
    "        charembed = s.srcembed(chars[i:i])\n",
    "        rnn_out = s.rnn(charembed)\n",
    "        starting_index += 1\n",
    "    end\n",
    "\n",
    "    for i in starting_index:maxlength\n",
    "        charembed = s.srcembed(chars[i:i])\n",
    "        rnn_out = s.rnn(charembed)\n",
    "        output = s.projection(dropout(rnn_out, s.dropout))\n",
    "        push!(chars, s.srccharset.c2i[ sample(s.srccharset.i2c, Weights(Array(softmax(reshape(output, length(s.srccharset.i2c)))))) ] )\n",
    "\n",
    "        if chars[end] == s.srccharset.eow\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "\n",
    "    join([ s.srccharset.i2c[i] for i in chars ], \"\")\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation metrics\n",
    "\n",
    "Here we define a `loss(model, data)` which returns a `(Σloss, Nloss)` pair if `average=false` and\n",
    "a `Σloss/Nloss` average if `average=true` for a whole dataset.\n",
    "\n",
    "`report_lm(loss)` calculates character perplexity and bit-per-character metrics."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function loss(model, data; average=true)\n",
    "    mean([model(x,y) for (x,y) in data])\n",
    "end\n",
    "\n",
    "report_lm(loss) = (loss=loss, ppl=exp.(loss), bpc=loss ./ log(2))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training procedure\n",
    "Train our model using Adam optimizer and saving the best performing model on dev set."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function train!(model, steps, trn, dev, tst...)\n",
    "    bestmodel, bestloss = deepcopy(model), loss(model, dev)\n",
    "    progress!(adam(model, trn), steps=steps) do y\n",
    "        devloss = loss(model, dev)\n",
    "        tstloss = map(d->loss(model,d), tst)\n",
    "        if devloss < bestloss\n",
    "            bestloss, bestmodel = devloss, deepcopy(model)\n",
    "        end\n",
    "        println(stderr)\n",
    "        (trn=report_lm(tstloss), dev=report_lm(devloss))\n",
    "    end\n",
    "    return bestmodel\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# seed random generators for reproducability\n",
    "Random.seed!(123);\n",
    "\n",
    "# Define char set, batchsize and max sequence length\n",
    "char_set = \" #\\$&'*-./0123456789<>N\\\\abcdefghijklmnopqrstuvwxyz\"\n",
    "datadir = \"../data/ptb\"\n",
    "BATCHSIZE, MAXLENGTH = 16, 256\n",
    "\n",
    "@info \"Reading data\"\n",
    "charset = Charset(char_set)\n",
    "train_reader = TextReader(\"$datadir/train.txt\", charset)\n",
    "dev_reader = TextReader(\"$datadir/valid.txt\", charset)\n",
    "test_reader = TextReader(\"$datadir/test.txt\", charset)\n",
    "\n",
    "dtrn = DataIterator(train_reader, batchsize=BATCHSIZE, maxlength=MAXLENGTH)\n",
    "ddev = DataIterator(dev_reader, batchsize=BATCHSIZE, maxlength=MAXLENGTH)\n",
    "dtst = DataIterator(test_reader, batchsize=BATCHSIZE, maxlength=MAXLENGTH)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@info \"Initializing Language Model\"\n",
    "epochs = 10\n",
    "ctrn = collect(dtrn)\n",
    "trn = collect(flatten(shuffle!(ctrn) for i in 1:epochs))\n",
    "trnmini = ctrn[1:20]\n",
    "dev = collect(ddev)\n",
    "model = LModel(256, 256, charset; layers=2, dropout=0.2)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@info \"Starting training ...\"\n",
    "model = train!(model, length(ctrn), trn, dev, trnmini)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@info \"Finished training, Starting evaluation ...\"\n",
    "trnloss = loss(model, dtrn);\n",
    "println(\"Training set scores:       \", report_lm(trnloss))\n",
    "devloss = loss(model, ddev);\n",
    "println(\"Development set scores:    \", report_lm(devloss))\n",
    "tstloss = loss(model, dtst);\n",
    "println(\"Test set scores:           \", report_lm(tstloss))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sampling sentences from the model"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@info \"Sample sentences from the model\"\n",
    "print(\"enter prompt or leave empty for no prompt, CTRL+C to exit\")\n",
    "\n",
    "while true\n",
    "    print(\"prompt:\")\n",
    "    prompt = lowercase(readline())\n",
    "    println(generate(model; start=prompt, maxlength=1024))\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  },
  "kernelspec": {
   "name": "julia-1.5",
   "display_name": "Julia 1.5.3",
   "language": "julia"
  }
 },
 "nbformat": 4
}
